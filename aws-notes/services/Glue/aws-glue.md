# AWS Glue

## 概要
サーバーレスの ETL（Extract, Transform, Load）サービス
データの収集・変換・カタログ化・ロードを自動化して、分析や機械学習に使いやすい形に整える

**「AWS上のデータETL処理をサーバーレスでやってくれるサービス」**

## 主要機能

### 1. データカタログ
- Glue Data Catalog はメタデータ管理サービス
- S3やDynamoDBなどのデータを「データベース」「テーブル」として登録
- Athena, Redshift Spectrum, EMR からそのまま利用可能

### 2. ETL ジョブ
- Python（PySparkベース）や Scala で書けるスクリプトを Glue が自動生成可能
- データを抽出（Extract）→ 変換（Transform）→ ロード（Load）して S3 や Redshift に格納

### 3. Glue Studio
- GUIベースでドラッグ＆ドロップのデータ変換ワークフローを構築できる
- コードを書かなくてもETLが可能

### 4. Glue Crawler
- S3 や DynamoDB などをスキャンして自動でスキーマを推論
- Data Catalog に登録

### 5. Glue Workflow / Trigger
- バッチ処理を スケジュール実行（毎日/毎週など）できる
- 複数ジョブを連携したワークフロー管理も可能

## ユースケース
- データレイク構築（S3に集めたデータを整形してカタログ化）
- データ統合（DynamoDB → S3、RDS → Redshift など）
- 分析前処理（AthenaやQuickSightで分析しやすい形式に変換）
- 機械学習前処理（特徴量抽出やデータクレンジング）

## メリット
- **サーバーレス**：インフラ管理不要、必要なときにだけリソースが動く
- **統合性**：Athena, Redshift, EMR, SageMaker などとシームレスに連携
- **自動化**：スキーマ推論・スクリプト自動生成で楽にETL開始できる

---

# データ移行での活用

## 基本は ETL サービス
Glue は本来 ETL（Extract, Transform, Load） を目的に作られたサービス
- ソースからデータを抽出 → 必要に応じて変換 → ターゲットにロード

## データ移行にも活用可能
「変換なしで移す」＝ シンプルなETL（＝EL） として使える

### 例
- DynamoDB → S3（アーカイブ/レポート用）
- RDS / Aurora → Redshift（分析基盤への移行）
- オンプレDB → AWS（S3やRedshift）（マイグレーション）

### 向いている移行パターン
- バッチ的な移行（定期的・大量データのコピー）
- S3やRedshift、DynamoDBなどAWSネイティブサービス間の移行
- データを移しつつ CSV/Parquet形式に変換 して格納する場合

### 注意点
- リアルタイム移行（ストリーミング移行） には向かない
- その場合は DMS (Database Migration Service) や Kinesis Data Streams / Firehose が適任
- Glue は バッチETL として位置づけるのが正解

---

# Glue Crawler

## 概要
S3, DynamoDB, RDS などのデータソースをスキャンして、自動でスキーマを推論し、Glue Data Catalog にテーブル定義を登録する仕組み

**「データの中身を見て、テーブル定義を自動で作る係」**

## 仕組み
1. クローラに「データソース（S3バケット、DynamoDBテーブルなど）」を指定
2. クローラがデータをスキャン
   - ファイル形式（CSV, JSON, Parquet など）
   - カラム名、データ型、パーティション構造
3. Glue Data Catalog にテーブルが自動作成される
4. Athena や Redshift Spectrum からそのまま SQL でクエリできる

## 役割の違い

### Glue クローラ
- データの「スキーマ推論」と「カタログ登録」
- データそのものは移動しない

### Glue ジョブ
- データを抽出・変換・保存（ETL処理）
- 実際の「データ移行・変換」はこっち

## ユースケース
- 新しいデータが S3 に来るたびにクローラでスキーマを更新
- DynamoDB テーブルをクローラでカタログ化して Athena で直接クエリ
- ETL ジョブを作る前にクローラでスキーマを把握

**ジョブは移行できる。クローラは移行はできない（カタログ化だけ）**

---

# スケジュール実行

## 1. Glue ジョブのテンプレート利用
Glue には「よくあるデータ転送処理」のテンプレートが用意されている

### 例
- DynamoDB → S3、RDS → S3、S3 → Redshift

### 特徴
- テンプレートをベースにすれば、コードをイチから書かなくてもデータ移行フローを作成可能
- 作ったジョブは スケジュール実行（毎週・毎日など） もできる

**Glue ジョブ = データを実際に移動する部分（ETLの実行者）**

## 2. Glue クローラのスケジュール実行
クローラは「データの中身をスキャンしてスキーマを Glue Data Catalog に登録」する役目

### 実行方式
- オンデマンド実行 または スケジュール実行 が可能
- 例: 毎週月曜 1:00 に S3 バケットをスキャンして新しいファイル形式をカタログ化
- 頻度・曜日・時間を細かく指定できる

**Glue クローラ = データの目録作成係（カタログ更新を自動化）**

## 3. DynamoDB → S3 → レポート生成の例
1. 毎週ジョブで DynamoDB → S3 にデータを移行（ETL）
2. その後クローラをスケジュール実行して S3 のスキーマをカタログに登録
3. Athena からレポートSQLを実行可能

## まとめ
-  **Glue ジョブ** → データ移行・変換の実行（テンプレート利用もOK、スケジュールも可）
-  **Glue クローラ** → スキーマのカタログ化（スケジュールで自動更新できる）
- 両方とも「オンデマンド or スケジュール実行」に対応

## Glue の場合
Glue ジョブもクローラも、サービス内に「スケジュール機能」が組み込まれている

EventBridge を直接設定しなくても、Glue のコンソールから「毎日/毎週/cron指定」でトリガーできる

**Glue = ETL処理 ＋ スケジュール管理まで内蔵**

- Lambda でやる場合 → EventBridge とセットで使う必要あり
- Glue でやる場合 → Glue 自体にスケジュール実行機能があるので単独で完結できる