# AWS データ分析・ETL・バッチ処理まとめ

## 全体像

AWSのデータ処理基盤は大きく分けて4段階

| フェーズ | 目的 | 代表サービス |
|---------|------|------------|
| E（Extract）抽出 | データを取り込む | Kinesis, Firehose, DMS |
| T（Transform）変換 | データを整形・加工 | Glue, EMR, Lambda |
| L（Load）格納 | 分析しやすい形に格納 | S3, Redshift |
| A（Analyze）分析 | SQL・BIで分析 | Athena, Redshift, QuickSight |

---

## EMRのスケジュール実行方法

EMR単体ではスケジュール機能がない

外部連携で自動化する

```
EventBridge → Step Functions → EMRクラスター起動
                                   ↓
                            Sparkジョブ実行
                                   ↓
                            クラスター停止
```

## SAMLアサーションとは

SAMLアサーション（SAML Assertion）は、SAML認証で「ユーザーが誰で、どんな権限を持っているか」を証明する署名付きXMLデータ

### SAML認証の構成要素

SAML認証は3者で成り立つ

| 役割 | 名前 | 役目 |
|-----|------|------|
| ユーザー | Principal | ログインする本人 |
| IdP（Identity Provider） | 認証サーバ（例：社内のADFSやAzure ADなど） | 「この人は確かに本人」と証明する |
| SP（Service Provider） | サービス側（AWS, Google Workspaceなど） | 「本人確認できたならログインさせる」 |

### SAMLアサーションの中身

```xml
<saml:Assertion>
  <saml:AttributeStatement>
    <saml:Attribute Name="https://aws.amazon.com/SAML/Attributes/Role">
      <saml:AttributeValue>arn:aws:iam::123456789012:role/SAMLRole,arn:aws:iam::123456789012:saml-provider/CorpIdP</saml:AttributeValue>
    </saml:Attribute>
    <saml:Attribute Name="https://aws.amazon.com/SAML/Attributes/RoleSessionName">
      <saml:AttributeValue>username@example.com</saml:AttributeValue>
    </saml:Attribute>
  </saml:AttributeStatement>
</saml:Assertion>
```

ここに書いてあるのが

- この人は username@example.com というユーザー
- AWS の SAMLRole を使う権限を持っている
- その認証は CorpIdP という IdP によって署名されている

### AWSフェデレーションでの重要ポイント

AWS側では、このアサーションの中のRole 属性（arn:aws:iam::...:role/...）を見て、「この人にはこのIAMロールをAssumeする権限がある」と判断する

CLIでは --role-arn を自分で指定するが、コンソールではこのSAMLアサーション内の情報をAWSが読み取るため、もしここが正しくないとログインに失敗する

### 用語の違い

- **SAML（サムル）** は「仕組み・プロトコルの名前」
- **SAMLアサーション（Assertion）** は「SAMLが使うデータの中身」

---

## SAMLとは

Security Assertion Markup Language の略

XMLベースの認証・認可のためのプロトコル（仕組み）

### 目的

「異なるシステム間で安全にユーザー認証情報をやりとりする」

### 例

社内のログイン情報（IdP）で外部サービス（AWS, Salesforce, Googleなど）に追加ログインなしでアクセスできるようにする仕組み

これを **SAML認証（SAML Federation）** と呼ぶ

### SAMLアサーション（SAML Assertion）とは

SAMLがやりとりする「実際の認証メッセージ」

| 項目 | 内容 |
|-----|------|
| 形式 | XMLデータ |
| 送信元 | IdP（Identity Provider） |
| 宛先 | SP（Service Provider） |
| 内容 | ユーザー名、所属グループ、ロール、署名などの情報 |

いわば「この人は確かに本人」という「電子の証明書」

---

## 通常のAssumeRoleとの違い

| 種類 | 実行するAPI | 証明に使うもの | どんなときに使う |
|-----|-----------|--------------|---------------|
| 通常のAssumeRole | sts:AssumeRole | IAMユーザーのアクセスキー | AWS内で別ロールに切り替える |
| SAMLフェデレーション | sts:AssumeRoleWithSAML | SAMLアサーション（IdP発行のXML） | 社内ADなど外部IdPからログイン |

SAMLフェデレーションでAssumeRoleするためには、IAMロールの「信頼ポリシー（Trust Policy）」に Principal としてSAMLプロバイダーを設定する必要がある

---

## タグポリシーとAWS Configの役割分担

| 比較項目 | タグポリシー（Tag Policy） | AWS Config ルール |
|---------|--------------------------|------------------|
| 主目的 | タグ命名・値の基準を決める（ポリシー定義） | その基準を守ってるか評価する（コンプライアンス確認） |
| 作用範囲 | AWS Organizations全体（複数アカウント） | 各アカウント単位（Config有効化済み） |
| 評価タイミング | 組織全体に自動的に適用（定期的評価） | Configがリソース変更を検知したときに評価 |
| 通知手段 | EventBridge（Tag Policy Compliance Event） | EventBridge、SNS、または自動修復 |
| Lambda必要？ | 不要（ネイティブ） | カスタムルールなら必要 |
| リソース作成を拒否できる？ | できない（監査のみ） | 基本はできない（同じく監査） |
| おすすめ用途 | 「どんなタグを使うべきか」を全社統一 | 「実際にそのタグが付いてるか」を自動チェック |

---

### タグポリシーで"ルール"を決める

「全社的に使えるタグはこれだけ」を定義

```json
{
  "tags": {
    "CostCenter": {
      "tag_key": {
        "enforced_for": ["ec2:instance", "s3:bucket"]
      },
      "tag_value": {
        "allowed_values": ["FIN", "MKT", "DEV"]
      }
    }
  }
}
```

### AWS Configで"実際に守られてるか"を監視する

Configルールを有効化して

- リソースに CostCenter タグがない場合「非準拠」
- 値が FIN/MKT/DEV 以外なら「非準拠」

に設定する

違反したら EventBridge → SNS → チーム通知、あるいは Systems Manager Automationで自動修正も可能

---

## どっちを選ぶべきか（設計判断）

| ケース | 適した仕組み |
|-------|------------|
| タグ命名規則を一元管理したい | タグポリシー |
| タグルールを実際にチェックしたい | AWS Config |
| タグ違反を検知したら即通知したい | AWS Config + EventBridge |
| Lambda禁止・AWS標準機能のみ | タグポリシー |
| 修復や強制適用までしたい | AWS Config + Automation |

---

## タグポリシーには「イベント通知機能」がない

タグポリシーは EventBridge と直接連携できない

つまり、「非準拠リソースを検出 → SNSで通知」というフローは構築できない

（※一応 Organizations からコンプライアンス状態を EventBridge 経由で見る方法はあるが、"リアルタイム検知"ではなく定期スキャンベース）

### 対して AWS Config は

AWS Config は「リソースの状態監視＋評価＋イベント連携」を専門にしているサービス

例えばマネージドルール：`required-tags`

このルールを設定して "Confidential" タグを必須にすれば

1. バケットにタグがないと「NON_COMPLIANT」
2. そのイベントを EventBridge → SNS に通知
3. 必要なら自動修復（Systems Manager Automation 連携）

が完全に自動でできる

---

## 役割分担まとめ

| 機能 | タグポリシー | AWS Config |
|-----|------------|-----------|
| タグの命名・値制限ルールを定義 | できる | できる（ルールを自作すれば） |
| 違反を検出（非準拠判定） | 定期スキャン | 即時検出（Configルール） |
| 通知（SNS, EventBridge） | できない | できる |
| 修復・自動対応 | できない | できる |
| Lambda禁止でも動く | OK | マネージドルールならOK、カスタムはNG |

### 結論

- タグポリシーは"ルールブック"
- AWS Configは"監視員"

---

## タグポリシーの定義例

```json
{
  "tags": {
    "CostCenter": {},
    "Environment": {},
    "Owner": {}
  }
}
```

### この定義の意味

- 「CostCenter」「Environment」「Owner」だけが"推奨タグ"
- 「それ以外のタグキー（例：Team, Project）」を使っても構わないが、"非準拠"として扱う

### 例

誰かがこんなタグをつけた場合

| タグキー | 値 | 結果 |
|---------|---|------|
| CostCenter | FIN | 準拠 |
| Owner | user@example.com | 準拠 |

---

## AWS Config（マネージドルール）で同等のことをやる場合

Configは "required-tags" というマネージドルールを使う

設定内容はJSONというよりパラメータ指定

```bash
aws configservice put-config-rule \
  --config-rule-name "require-tags" \
  --source "Owner=AWS,SourceIdentifier=REQUIRED_TAGS" \
  --input-parameters '{"tag1Key":"CostCenter","tag2Key":"Environment","tag3Key":"Owner"}'
```

### つまり Config では

- tag1Key, tag2Key, tag3Key のように キーごとに指定
- 「JSONでタグ定義」ではなく「ルールにパラメータ渡す」方式

### まとめ

- タグポリシーは「JSONでルールを宣言する」
- AWS Configは「コードやルールで評価・修復する」

---

## required-tags の動き（標準マネージドルール）

このルールがやってくれるのは「タグ監査」だけ

中身は単純にこんな動き

1. Configがリソースのタグを読み取る
2. input-parameters で指定したキーがあるか確認
3. 足りなければ NON_COMPLIANT（非準拠）と判定

### 例：CostCenter / Environment / Owner タグが必須

```bash
aws configservice put-config-rule \
  --config-rule-name "required-tags" \
  --source "Owner=AWS,SourceIdentifier=REQUIRED_TAGS" \
  --input-parameters '{"tag1Key":"CostCenter","tag2Key":"Environment","tag3Key":"Owner"}'
```

### 結果

| リソース | タグ | 判定 |
|---------|-----|------|
| EC2 A | CostCenter, Owner, Environment | 準拠 |
| EC2 B | Owner だけ | 非準拠 |
| S3 C | タグなし | 非準拠 |

---

## 「修復したい」ならどうする

### 方法1：カスタムルール（Lambda）

Lambda内で「タグを修正・追加」処理を書く

#### 例：typoタグ自動修正

```python
# -*- coding: utf-8 -*-
# AWS Config Custom Rule: Fix tag typo

import boto3, json
ec2 = boto3.client("ec2")

def lambda_handler(event, context):
    invoking_event = json.loads(event["invokingEvent"])
    config_item = invoking_event["configurationItem"]
    resource_id = config_item["resourceId"]
    tags = config_item.get("tags", {})

    # Typoを修正
    if "Environmet" in tags:
        ec2.create_tags(
            Resources=[resource_id],
            Tags=[{"Key": "Environment", "Value": tags["Environmet"]}]
        )
        ec2.delete_tags(Resources=[resource_id], Tags=[{"Key": "Environmet"}])

        return {
            "compliance_type": "NON_COMPLIANT",
            "annotation": "Fixed typo tag 'Environmet' to 'Environment'."
        }
    return {"compliance_type": "COMPLIANT"}
```

これでConfigが違反を検知した瞬間にLambdaが呼ばれて、タグを修復できる

### 方法2：自動修復（Remediation）＋SSM Automation

AWS Configには「修復アクション（Remediation Action）」機能があり、非準拠を検出したら Systems Manager Automation ドキュメントを実行する設定も可能

#### 例

1. 非準拠イベント → EventBridgeでトリガー
2. AWS-ApplyTag Automation ドキュメントを起動してタグを付与

これで「タグ付け忘れを自動補完」も可能

---

## まとめ

| やりたいこと | 使う仕組み |
|-----------|----------|
| タグが無いリソースを検出したい | required-tags マネージドルール |
| タグのtypoを修復したい | Configカスタムルール（Lambda） |
| タグ付け忘れを自動補完したい | Config Remediation + SSM Automation |
| タグキー・値ルールを全社統一したい | タグポリシー（Organizations） |
| タグが無いリソース作成をブロックしたい | SCP or IAMポリシー |

---

## つまりFSxは「マネージドWindowsファイルサーバー」

FSx（特に FSx for Windows File Server）は、AWSが裏でWindows Serverを起動してくれて、「共有フォルダ（\share）」を使うだけ、という構成

### ユーザー目線では

- EC2を立てない（裏では立ってるけど管理不要）
- OSパッチ適用、冗長構成、フェイルオーバーなども自動
- アクセスは普通に SMB プロトコル（\fsx.example.com\share）で可能

つまり「EC2のように直接触れないけど、VPC内に存在するWindowsサーバー」

---

## FSx for Windows File Server の選択肢

FSxの中にはいくつか製品ファミリーがあるが、「Single-AZ」と「Multi-AZ」が選べるのはこの Windows File Serverタイプ

| FSxの種類 | 主な用途 | AZ構成の種類 |
|----------|---------|------------|
| FSx for Windows File Server | SMB共有（Windows用） | Single-AZ / Multi-AZ |
| FSx for Lustre | HPC・機械学習 | Single-AZのみ |
| FSx for NetApp ONTAP | 高機能ストレージ（SnapMirrorなど） | Multi-AZ（HAペア構成） |
| FSx for OpenZFS | Linux環境・ZFS機能 | Single-AZ / Multi-AZ |

### FSx for Windows File Server の設定

| 対象 | 内容 |
|-----|------|
| デプロイタイプ | Single-AZ または Multi-AZ（２択） |
| 変更可否 | 後から変更不可 |
| Multi-AZの特徴 | 冗長構成・自動フェイルオーバーあり |
| Single-AZの特徴 | 単一AZ・安価・シンプル構成 |

FSxのデプロイタイプ（Single-AZ ⇔ Multi-AZ）は作成後に変更できない仕様

---

## なぜ同期維持できないのか

復元中は15TBのリストアで数時間以上かかるし、同期を維持できない = 「ダウンタイム1時間以内」にはおさまらない

### 結論

FSxのバックアップから復元する方式では「増分同期（差分コピー）」ができないから、データが静的スナップショット時点で"固定"されてしまい、復元中に元のFSxで更新されたファイルは反映されない — つまり「同期が維持できない」

### ① FSxのバックアップとは

FSxのバックアップは、AWS内部で**スナップショット（静止点コピー）**を取る仕組み

取得した時点の状態を静的に保存する

これは「その瞬間の完全コピー」であり、「継続的な同期」ではない

- 時刻T0 → バックアップ開始（15TBのスナップショット）
- 時刻T1 → バックアップ完了（数時間後）

この間に、既存のSingle-AZ FSxでファイルが更新されても、スナップショットには反映されない

### ② 復元（リストア）の仕組み

バックアップを新しいMulti-AZ FSxに復元するときも、「バックアップ時点の状態」から作られるため

- 復元されたファイルサーバーは "過去時点のコピー"
- 元のFSxとは別リソース（別ID・別DNS名）
- 両者の間にリアルタイム同期機能は存在しない

つまり、復元が終わるまでに元のFSxで更新されたデータ（例：ユーザーが保存したファイル）は、新しい方には入ってこない

### ③ なぜ「ダウンタイム1時間以内」が難しいか

15TBのFSxを復元するとき

- AWSの内部スナップショット復元でも数時間単位でかかる（数十TB級は普通に6〜10時間クラス）
- しかも復元後のFSxと既存のFSx間には同期経路がない

→ だから、「旧→新」への切り替えタイミングでデータの不整合リスクが発生

→ そのため、メンテナンスを長時間止める（または手動コピーが必要）= ダウンタイム増大

---

## 対して「AWS DataSync」はどう違うのか

DataSyncはリアルタイム or 定期同期をサポートする

つまり以下のような構成が取れる

```
Single-AZ FSx (稼働中)
   ↓ DataSync (増分同期)
Multi-AZ FSx (新規構築)
```

これなら

- 初回フルコピー後は「差分同期（変更分だけ転送）」できる
- 切り替え直前に最終同期を走らせる
- 1時間以内にDNSを切り替えるだけで済む
- = ダウンタイムを最小化できる

### まとめ

| 比較項目 | バックアップ復元方式 | DataSync方式 |
|---------|------------------|-------------|
| データ転送 | スナップショット（静的コピー） | 増分同期（リアルタイムまたは定期） |
| 同期維持 | 不可（復元時点で固定） | 可（差分更新を継続） |
| 所要時間 | 数時間〜数十時間 | 初回＋差分で短時間 |
| ダウンタイム | 長い（数時間以上） | 短い（最終同期＋切替のみ） |
| データ整合性 | 保てない（更新ロスあり） | 保てる（同期で整合） |

---

## バックアップ vs レプリケーション

- 「バックアップ（snapshot / restore）」は"静止点コピー"であり、同期を維持できない
- 「レプリケーション（replication / sync）」は継続的な差分同期ができる

### AWSサービスごとの比較例

| サービス | バックアップ方式 | 同期できる？ | 同期系の別サービス・機能 |
|---------|--------------|-----------|---------------------|
| EBS | Snapshot（S3に保存） | 復元は静止点 | EBS Replication（2023年以降対応） |
| RDS / Aurora | バックアップ or スナップショット | 復元は別インスタンス | Cross-Region / Read Replica（継続同期） |
| S3 | Versioning + Backup | 単発コピー | Cross-Region Replication（継続同期） |
| FSx for Windows | バックアップ（静的コピー） | 同期維持なし | AWS DataSync（増分同期） |
| EFS | Backup（AWS Backup） | 同期不可 | EFS Replication（数分遅延で自動同期） |
| DynamoDB | Point-in-time recovery | 静止点 | Global Tables（リアルタイム複製） |
| Redshift | Snapshot | 同期なし | Cross-Region Snapshot Copy（スケジュール転送） |
| EC2 AMI | 静的イメージ | 同期不可 | DRS（Disaster Recovery Service）で近似レプリケーション可 |

---

## AWSの考え方

AWSは内部的に「データ保護のレイヤー」を明確に分けている

| 機能の分類 | 主目的 | タイプ | 特徴 |
|----------|-------|-------|------|
| Backup / Snapshot | 復旧用（履歴・静止点） | 静的コピー | 同期なし・復元に時間がかかる |
| Replication / Sync | 可用性・フェイルオーバー用 | 動的同期 | 継続的差分・整合性維持 |
| DR（Disaster Recovery） | 両者の中間 | 定期レプリケーション | 差分転送＋迅速復旧を目指す |

---

## EBS

Snapshotを取ると、S3上に"時点コピー"が保存される

その後EC2でファイルを書き換えても、Snapshotは更新されない

→ 同期がない（完全に静止）

### 一方で

EBS Volume Replication（2023追加機能）は、リアルタイムで別AZに差分同期される

→ 同期がある

---

## RDS / Aurora

- **DB Snapshot**: その時点のバックアップ → 復元すると新しいDBが作られる（同期なし）
- **Read Replica / Aurora Replica**: トランザクションログを継続的に転送（同期あり）

つまり、RDSスナップショットで復元しても、元DBで更新されたデータは反映されない

---

## S3

S3単体では「同期」はない（ただしVersioningで過去データ保持は可能）

Cross-Region Replication (CRR) を設定すると、オブジェクトの追加・更新がリアルタイムで他リージョンに反映される

---

## EFS

EFSのバックアップも「静止点コピー（同期なし）」だが、**EFSレプリケーション（Replication）**を使えば自動・継続的に同期（増分コピー）できる

つまりEFSは以下のように両方持っている

| 機能 | タイプ | 同期維持 | 目的 |
|-----|-------|---------|------|
| AWS Backup / EFS Backup | バックアップ（静止点コピー） | なし | データ保護・復旧用 |
| EFS Replication | 継続レプリケーション | あり | 可用性・DR用 |

### ① EFS Backup（同期なし）

AWS Backup（またはEFS内蔵の自動バックアップ）を使って指定時点のスナップショットをS3に保存

静止点コピーなので、「バックアップを取った後に更新されたファイル」は復元先EFSに反映されない

つまり

- 「過去に戻す」用途にはOK
- 「同期維持」には不向き

復元には通常数時間単位かかる（容量依存）

### ② EFS Replication（同期あり）

EFSの 「Replication」機能（2022年リリース）は、EFS同士を自動で継続的に差分同期してくれる

#### 特徴

| 項目 | 内容 |
|-----|------|
| 同期単位 | EFS → 別リージョン または 同一リージョン |
| 同期方式 | 非同期（数分遅延） |
| データ転送 | 増分のみ |
| RPO（目標復旧時点） | 約5〜15分程度 |
| RTO（切替時間） | 数分（DNS切替など） |
| 用途 | DR対策・高可用性構成 |

### EFS Replicationとの違い

| 機能 | DataSync | EFS Replication |
|-----|---------|----------------|
| 同期方式 | 手動 or スケジュール（バッチ） | 自動・継続的 |
| リアルタイム性 | 数時間〜任意間隔 | 数分遅延（5〜15分） |
| 監視・差分検出 | ファイル属性比較 | ブロックレベル（内部差分） |
| 双方向 | なし | なし（片方向） |
| 運用コスト | タスクごとに課金 | EFSの追加コストのみ |
| 信頼性 | 管理者スクリプト次第 | AWSが冗長管理 |

---

## FSx vs EFS の同期機能

FSx for Windows File Server には、EFS Replication のような「ネイティブな継続同期機能（Replication）」が存在しない

だから、現場では「代わりに AWS DataSync を使って同期っぽくする」しかない

- **EFS** → Replication（自動）
- **FSx** → DataSync（手動で同期を模倣）

### デプロイタイプ比較

| サービス | デプロイタイプ（構成の種類） |
|---------|------------------------|
| FSx for Windows File Server | Single-AZ / Multi-AZ（選択式） |
| EFS（Elastic File System） | Multi-AZのみ（自動冗長） |

---

## EFS の冗長性

EFSは、1つのEFSファイルシステムを作成すると、AWS内部で自動的に 複数のAZにまたがってデータを複製

### Standard vs One Zone

| モード | 冗長性 | コスト | 可用性 | 主な用途 |
|-------|-------|-------|-------|---------|
| EFS Standard | Multi-AZ（デフォルト） | 高い | 99.99% | 本番運用・DR |
| EFS One Zone | Single-AZ（冗長なし） | 安い | 99.9% | キャッシュ・一時データ・検証環境 |

---

## AWS Global Accelerator（GA）

- 各AZのエンドポイント（EC2/NLBなど）を登録可能
- 固定IP（Anycast IP） を自動で2つ払い出し
- 各リージョン/AZへ最適経路でルーティング
- TCP/UDP両方OK（HTTP以外のポートにも対応）

---

## ALB vs NLB vs CLB

ALB（Application Load Balancer）は TCP:7000 のような「L4レベルの通信」は扱えない

ALBは HTTP / HTTPS / gRPC（=L7）専用のロードバランサー

TCPポート7000のような**任意ポート通信（HTTP以外）**を扱うなら、Network Load Balancer（NLB） か Global Accelerator を使う必要がある

### ロードバランサー比較

| 種類 | 層 | プロトコル | 使えるポート | 主な用途 |
|-----|---|-----------|------------|---------|
| ALB (Application Load Balancer) | L7 (アプリ層) | HTTP / HTTPS / gRPC | 80, 443（基本） | Webアプリ、API、gRPC |
| NLB (Network Load Balancer) | L4 (トランスポート層) | TCP / UDP / TLS | 任意ポートOK（例：7000） | DB接続、非HTTP通信、専用TCPサービス |
| CLB (Classic Load Balancer) | L4/L7混在（旧式） | HTTP / HTTPS / TCP | 任意ポート | レガシー用途（非推奨） |

---

## NLB＋EIP＋Route 53 構成

```
Internet
   │
Route 53 (Aレコード)
   │
NLB@AZ-a  ← EIP-1
NLB@AZ-b  ← EIP-2
   │
EC2インスタンス (TCP:7000)
```

### 仕組み

- 各AZごとにNLBを立て、**それぞれにElastic IP（固定IP）**を割り当て
- Route 53でAレコードを登録（例：quotes.finance.example.com → 両EIP）

これにより:
- 固定IPを取引先に提示できる
- 複数AZに分散可能
- TCP:7000も扱える（L4通信OK）

### メリット

- シンプル・安価
- 完全固定IP（EIP）でファイアウォール登録しやすい
- Route 53でフェイルオーバー設定も可能

### デメリット

- DNSフェイルオーバーは数十秒〜数分の遅延がある
- Anycastではない（地域最適化されない）
- 接続先によっては非効率ルートを通る

## AWS Global Accelerator 構成

```
Internet
   │
Global Accelerator (Anycast IP ×2)
   │
   ├─ Endpoint Group (AZ-a EC2)
   └─ Endpoint Group (AZ-b EC2)
```

### 仕組み

- Global AcceleratorがAnycast IPを2つ発行（固定）
- 世界中のAWSエッジロケーションでトラフィックを受け取り、最適な経路で目的リージョンのエンドポイント（EC2/NLB）へ転送
- TCP/UDPすべてのポート対応（7000もOK）
- AZ障害時はミリ秒単位で自動フェイルオーバー

### メリット

- リアルタイムなヘルスチェック＋自動フェイルオーバー
- グローバル最適ルーティングで低レイテンシ
- **固定IP（Anycast）**を全世界で共通利用
- DNS TTL問題なし（IPは変わらない）

### デメリット

- 料金が少し高い（月額 + 転送料）
- リージョン内だけで済む構成では少しオーバースペックになる場合も

### 80/443以外 → NLB or Global Acceleratorかもとなれ

## ALBはDNSベースで動くから

ALBのエンドポイントは、実は「IPアドレス」ではなく「DNS名」で提供される

```
my-app-alb-1234567890.ap-northeast-1.elb.amazonaws.com
```

このDNS名の裏には、複数のAZに分散配置されたIPアドレスが隠れている。しかもこれらのIPはAWS内部で自動的に変わることがある。

したがって:
- 固定IPを前提にしたファイアウォール設定や
- 取引先への「このIPに通信してください」みたいな要件

には、ALBは不向き

## なぜAWSはALBに固定IPをつけないのか

ALBは レイヤー7（HTTP/HTTPS） ロードバランサーで、内部的にAZごとの「ENI（Elastic Network Interface）」を動的に増減させている。

つまり:
- スケールアウト／インに合わせてIPが動的に増える
- AZ追加・削除でIPも変わる → 固定できない設計

Application Load Balancer には Elastic IP アドレスを関連付けることはできない。IP アドレスは動的に割り当てられる。

### 「固定IPっぽく」見せる方法

① AWS Global Accelerator を ALB の前に置く

```
Client → Global Accelerator (固定Anycast IP)
        → ALB (L7)
        → EC2 / ECS
```

Global Accelerator が「固定IP」を提供し、その裏で ALB の DNS にルーティング

- これで ALBでも固定IPアクセスが可能
- DNSを公開せずに済む
- フェイルオーバーも超速い

## NLBでは固定IPを使える理由

NLBは レイヤー4（L4：TCP/UDP/TLS） のロードバランサーで、各アベイラビリティゾーン（AZ）ごとに「実体のENI（Elastic Network Interface）」を持っている。

そのため、そのENIに対して Elastic IP（EIP）＝固定グローバルIP を関連付けることが可能

---

## Aurora Global Database vs RDS Cross-Region Read Replica

| 用語 | Aurora Global Database | RDS Cross-Region Read Replica |
|------|------------------------|------------------------------|
| レプリケーション方式 | Aurora専用の物理レプリケーション | MySQLのbinlogレプリケーション |
| 遅延 | 数百ミリ秒〜1秒 | 数十秒〜数分 |
| フェイルオーバー | 自動 | 手動 |
| RTO/RPO | <1分 / <1秒 | >10分 / 数十秒〜分 |
| 運用負荷 | 最小 | 手動介入あり |

---

## Aurora Multi-AZ vs Aurora Global Database

| 項目 | Aurora Multi-AZ | Aurora Global Database |
|------|----------------|----------------------|
| 範囲 | 同一リージョン | 複数リージョン |
| 同期/非同期 | 同期 | 非同期（高速物理） |
| RPO | 0秒（同期書き込み） | <1秒 |
| RTO | 数十秒 | <1分 |
| 自動フェイルオーバー | あり | あり（昇格＋DNS切替） |

「Multi-AZは可用性」「Replicaはスケール」「Globalは災害復旧」

---

## データ形式

CSV形式はコスト高・分析非効率（列指向ではない）

---

## ざっくりの判断基準（AWS公式＋実務感）

| 帯域（目安） | 推奨構成 | 理由・特徴 |
|------------|---------|----------|
| 〜100 Mbps | Site-to-Site VPN（IPSec） | 小規模・PoC・テスト・バックアップ用途。構築が即日可能 |
| 100〜500 Mbps | VPN ×2（冗長）または DX + VPN 併用 | VPNでもいけるが、ピーク時遅延が気になる帯域。冗長性を強化 |
| 500 Mbps〜1 Gbps | Direct Connect（1 Gbpsポート） | 帯域確保・安定性重視。VPNではジッター・遅延・再送が増える |
| 1〜10 Gbps | Direct Connect 専用線（1G/10G/100Gポート） | 大規模データ転送・バックアップ・VDI・DWH用途。SLAが明確 |
| 10 Gbps以上 | Direct Connect + Transit Gateway + LAG構成 | 複数DXでロードバランス。金融・製造・AI用途 |

---

## VPN vs Direct Connect 比較（400 Mbps想定）

| 観点 | VPNで400 Mbps | Direct Connectで400 Mbps |
|------|--------------|--------------------------|
| スループット | 理論上 1.25 Gbps まで出せるが、実際は ~300〜600 Mbps が安定限界 | 契約帯域保証（例：1 Gbps DX）で400 Mbps は余裕 |
| 遅延 | インターネット経由なので 20〜80 ms 揺れる | 数 ms レベルで安定（専用線） |
| ジッター（揺れ） | 大きい。リアルタイム通信やDB同期に不向き | 非常に小さい。レプリケーションやSAPに最適 |
| 可用性 | インターネット依存。混雑で帯域低下あり | SLA 99.9 %以上。帯域は保証される |
| コスト | 安い（月額 数千円 ＋ 通信量） | 高い（1 Gbps DX で 数万〜十数万円 ＋ 回線工事） |
| 構築時間 | 即日（数分） | 数週間〜数か月 |
| 暗号化 | IPSec 標準対応 | 暗号化なし（必要なら MACsec） |

---

## Transit Gateway構成

```
オンプレ（CGW複数）
   │
   │（Site-to-Site VPN トンネル複数）
   ▼
[AWS Transit Gateway]
   │
   ├─ VPC A（部門A用）
   ├─ VPC B（部門B用）
   ├─ VPC C（共通サービス用）
   └─ ・・・（最大5,000 VPC）
```

### キー概念

| 用語 | 役割 | 関係 |
|-----|------|------|
| CGW (Customer Gateway) | オンプレ側のVPNルータ（物理 or 仮想） | Transit Gatewayに接続される相手側 |
| VGW (Virtual Private Gateway) | VPC直結VPNのゲートウェイ（旧式） | VPC単位での接続に使用（今は非推奨） |
| Transit Gateway (TGW) | AWS側でVPC・VPN・DXを一元接続するハブ | VGWの上位互換・中央ハブ的存在 |

### 構成タイプ比較

| 構成タイプ | 接続形態 | 備考 |
|----------|---------|------|
| 旧式構成 | VPC ⇄ VGW ⇄ VPN ⇄ CGW | VPC単位でVPNを貼る必要あり（スケール悪） |
| 新構成（推奨） | 複数CGW ⇄ VPN ⇄ TGW ⇄ 各VPC | TGWで一括集約・ルート集中管理が可能 |

### ポイント

- CGWは複数あってOK（拠点ごとに異なるルータ）
- TGWは1つでOK（中央ハブ）
- VGWは基本もう使わない（古い構成）

---

## Direct Connect の場合

オンプレDCからDirect Connect Gateway（DXGW）を経由してTransit Gatewayにアタッチする構成も可能

```
オンプレDC ── DX ── DXGW ── TGW ── 各VPC
```

Direct Connect（DX）経由の場合は、CGW（Customer Gateway）は使わない

### CGW（Customer Gateway）の役割

| 要素 | 説明 | 通信プロトコル |
|-----|------|--------------|
| CGW (Customer Gateway) | オンプレ側のVPN終端ルータ（物理 or 仮想） | IPsec（VPN） |
| VGW / TGW | AWS側のVPN終端（VGWはVPC単位、TGWは集約ハブ） | IPsec（VPN） |

### DX構成で登場するリソース

| リソース | 役割 | 接続相手 |
|---------|------|---------|
| Direct Connect Gateway（DXGW） | AWSのDirect Connect用ハブ | Transit Gateway（TGW）やVPC |
| Transit Gateway（TGW） | 複数VPC・VPN・DXの集約ハブ | DXGWやVPC |
| オンプレDCルータ | 物理的なBGPピア（DX提供回線に接続） | AWS側DXルータ（物理） |

### 接続パターン

**VPN接続（CGWが必要）**

```
オンプレルータ（CGW）───VPN(IPsec)───Transit Gateway
```

**Direct Connect接続（CGW不要）**

```
オンプレルータ───物理専用線───AWS Direct Connect Router───DXGW───TGW───各VPC
```

Direct Connect の場合、オンプレ側の「物理ルータ（お客様設備）」が CGW の代わりの役割を果たす。ただし、AWS的にはそれを 「CGWリソース」としては登録しない。

- VPN＝CGWが必要
- DX＝物理接続だからCGW不要（物理ルータがその代わり）

---

## ACU = Aurora Capacity Unit

Aurora Serverless が自動でスケールする際の CPU・メモリ・ネットワークの複合的な単位

### 具体的に

Aurora Serverless では、利用量に応じて自動的にスケールする（常時稼働インスタンスはなし）

その「スケーリング単位」が ACU（Aurora Capacity Unit）

1 ACU はざっくり以下のようなリソース量に相当する

---

## API Gateway の3つのエンドポイントタイプ

| タイプ | ネットワーク経路 | 主な用途 | 補足 |
|-------|---------------|---------|------|
| Edge-Optimized | CloudFront POP（エッジロケーション）経由 | グローバルユーザー向け | CloudFrontを自動で裏側に使う |
| Regional | 直接リージョンのAPI Gatewayに接続 | 同一リージョン内ユーザー向け | 低レイテンシでシンプル構成 |
| Private | VPC内（PrivateLink経由）で接続 | 社内・閉域網用 | 外部公開されない |

---

## ステージとは

API Gatewayには「ステージ（Stage）」という概念がある

例:

| ステージ名 | 説明 |
|----------|------|
| dev | 開発用エンドポイント |
| staging | テスト用 |
| prod | 本番用 |

---

## ステージキャッシュとは

そのステージ単位でレスポンス結果をキャッシュできるのが「ステージキャッシュ（Stage Cache）」

つまり、「同じ入力のリクエストを何回も呼ばれたら、Lambdaを毎回叩かずに、前回のレスポンスを返す」という仕組み

### 動作イメージ

```
[1回目]
Client → API Gateway → Lambda → Aurora
                    ↑
           結果をキャッシュに保存

[2回目]
Client → API Gateway
           ↓（キャッシュHIT）
       Lambda・Auroraは呼び出さない
```

### TTL（Time To Live）

TTL値を設定可能（例：300秒）

TTLが切れると、再びLambdaを呼び出して結果を更新する

例:
- 「1分間は同じ結果を返してOK」→ TTL=60秒 に設定すればOK

---

## ① Tag Editor（タグエディター）

### 目的

既存リソースにタグを「付ける／編集する」ためのツール

### 機能の場所

AWS マネジメントコンソールの「Resource Groups」内

### できること

- 複数アカウント・リージョンにわたって、タグを検索・一括編集・追加
- 例：「DeptCode」タグを持たないRDSを検索して一括で追加する

### 対象

実際のリソース（EC2, RDS, S3, DynamoDBなど）

### 運用用途

- 既存リソースのタグを整える・修正する
- 組織的なタグ統一の「初期整備」や「定期メンテ」に向いている

つまり：Tag Editor = "タグを実際につけるツール"

---

## ② タグポリシー（Tag Policies）

### 目的

タグの命名ルールを「監視・統制」するための機能

### 機能の場所

AWS Organizations の「タグポリシー」機能

### できること

- 「このキー名はこう書くべし（例：DeptCode、AppStage）」をルール化
- 準拠していないタグをレポート化して検出

### 対象

ルールとレポート。実際のタグ付けは行わない

### 注意点

- タグポリシーは自動修正できない
- 違反を検出して「見える化」するだけ

つまり：Tag Policy = "タグ命名ルールをチェックする仕組み"

---

## タグ管理のベストプラクティス

1. Tag Editorで既存リソースのタグを整備する
2. SCPでタグなし作成を禁止する
3. Tag Policyでルール遵守状況を監視する

---

## Blue/Greenデプロイ時の仕組み

CodeDeployでは、次のような構成になる:

### 構成要素

- **ALB**: トラフィックの振り分け役
- **ターゲットグループ（TargetGroup）**:
  - Blue: 旧バージョン（現在稼働中）
  - Green: 新バージョン（これから切り替え）

### デプロイ時の動作順序

1. 新バージョンを Green側のAuto Scaling Group にデプロイ
2. ヘルスチェック（ALB, CloudWatchなど）で正常性確認
3. 問題がなければ、ALBのターゲットグループをBlue → Greenへ切り替え → トラフィックが新環境に流れる
4. 一定時間（terminationWaitTimeInMinutes）が経過後、古いBlue環境を削除

### ロールバック時の動作

もし CloudWatch アラームなどで異常を検知した場合:

1. ALBのターゲットグループを再び「Green → Blue」に戻す
2. 新しいGreen環境は切り離される（または停止状態に）
3. 旧バージョン（Blue）が再びトラフィックを処理するようになる

つまり:
- 「ロールバック」＝ ALBのターゲットグループの付け替え操作を自動で戻す

