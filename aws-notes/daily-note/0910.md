# NAS (Network Attached Storage)

## 概要
NASはネットワークに直接接続して利用するファイルストレージ専用機器である。

- NFS や SMB といったファイル共有プロトコルでアクセス
- ユーザーやアプリからはネットワーク上のファイルサーバとして見える
- 複数ユーザーが同時にファイルにアクセス可能

## 特徴
- 専用OSやストレージ機能を搭載
- 例：Synology、QNAP、NetApp Filer など
- ファイル単位でのアクセス（オブジェクトやブロックではなく）
- 簡単に共有可能
- Windowsの「\\NASサーバ\\共有フォルダ」やLinuxの /mnt/nas のようにマウント

## ユースケース
- 社内ファイルサーバ
- バックアップ置き場
- 映像・画像・ドキュメントの共有
- アプリケーションがNFS/SMB前提で動く場合のストレージ

---

# ストレージの種類との比較

## NAS (File Storage)
- **プロトコル**: SMB/NFS
- **単位**: ファイル
- **例**: 社内ファイル共有サーバ

## SAN (Storage Area Network, Block Storage)
- **プロトコル**: iSCSI, Fibre Channel
- **単位**: ブロックデバイス
- **例**: データベースサーバのストレージ

## オブジェクトストレージ
- **プロトコル**: HTTP(S) API (S3など)
- **単位**: オブジェクト（ファイル＋メタデータ）
- **例**: Amazon S3, Azure Blob

---

# まとめ

**NAS** = ネットワークにつないで複数人で使うファイルサーバ専用機器

- SMB/NFSでアクセスし、ファイル共有に便利

**File Gateway** = NASライクに見えるが、裏はS3に保存するサービス

プライベートサブネット内のAmazon RDSは、マネージドサービスとしてパッチの適用やデータ通信が自動的に管理されるため、NATゲートウェイは不要である。

## RDSとサブネットの基本

### RDSはマネージドサービス

OSパッチ適用やバックアップ、モニタリングはAWSが裏で処理する。そのためEC2のようにyum/aptで自分がパッケージを入れる必要はない。通常はプライベートサブネットに配置する。セキュリティ上、外部（インターネット）に直接出さないのがベストプラクティスである。

### NAT Gatewayは必要か？

RDS自体はNAT Gatewayを必要としない。RDSはユーザーがOSにログインしてパッケージを入れるような使い方はできない。AWSが裏でパッチやメンテナンスをやるときは、AWS内部の管理ネットワークを経由して行う。したがって「yum updateのためにNATが必要」ということはない。

### ただし注意点

RDSから外部サービス（例：外部API、サードパーティライセンスサーバ、KMSを別リージョンで使う等）にアクセスしたいケースはありえる。その場合はNAT GatewayやVPCエンドポイントが必要である。

現在はNAT Gatewayが推奨されている。NATインスタンスは非推奨である。

---

# SNSトピック

## SNSトピックとは

Amazon SNS (Simple Notification Service) の「トピック」は、メッセージの配信先をまとめる論理的なチャネル（ハブ）のことである。

## 仕組み

Publisher（発行者）がSNSトピックにメッセージを送信する（例：CloudWatch アラームが発火してSNSに通知を送る）。

Subscriber（購読者）がSNSトピックにサブスクライブしている：
- メール
- SMS
- Lambda 関数
- SQS キュー
- HTTP(S) エンドポイント
- Kinesis Firehose など

SNSトピックが、受け取ったメッセージを購読者全員に同時配信する。

トピック = 発行者と購読者をつなぐ中継点

## 例

アラーム通知のユースケース：
- Publisher: CloudWatch Alarm
- SNSトピック: Critical-Alarm-Topic
- Subscriber:
  - 運用チームのメールアドレス
  - PagerDuty（HTTPエンドポイント）
  - AWS Lambda（自動復旧処理用）

CloudWatchアラームが発火 → SNSトピックに通知 → すべてのサブスクライバーに一斉配信

## メリット

- 1回の送信で複数宛先に配布
- 疎結合（PublisherはSubscriberを直接知らなくてよい）
- 柔軟な通知方法（メール、SMS、Lambda、SQSなど）

```
           [Publisher]
                │
         (メッセージ送信)
                │
          ┌─────▼─────┐
          │   SNS Topic │
          └─────▲─────┘
                │
   ┌──────────┼───────────┐
   ▼            ▼           ▼
[Email]     [SQS Queue]   [Lambda]
[SMS]       [HTTP API]    [Firehose] ...
```

# 配信保障の考え方

## 基本概念
SNSはat least once delivery（少なくとも1回は配信）を保証する。

一度トピックに届いたメッセージは必ずどこかに配送される。ただし重複する可能性はある。

## サブスクライバーごとの配信保障

### 強い配信保証あり

#### Amazon SQS
- SNS → SQS 連携は少なくとも1回配送
- 配信失敗時も再試行あり
- 永続キューに入るため安全性が高い

#### AWS Lambda
- SNSからLambda呼び出しも少なくとも1回実行されるまでリトライ

### 弱めの配信保証

#### Email / SMS / HTTP(S) エンドポイント
- 配信は試みられるが、相手側が拒否したり落ちていたら失敗
- SNSは再試行（HTTPは最大数日間リトライ）するが、最終的に届かない可能性もある

## Dead-Letter Queue (DLQ)
- SNSには配信失敗時にメッセージを退避させる仕組み（DLQ: Dead Letter Queue）がある
- サブスクライバーに配信できなかったメッセージをSQSやLambdaのDLQに保存可能
- 運用チームはDLQを監視して、失敗メッセージを再処理できる

## まとめ
- SNSは「少なくとも1回は配送される」配信保障を提供
- 強い保証が欲しいならSQSやLambdaと組み合わせる
- 配信失敗を拾いたいならDLQを設定するのがベストプラクティス

---

# AWS Global Accelerator

AWS Global AcceleratorはALB/NLBのフロントに置けるサービスである。

ただし実際にはNLBと組み合わせるのがベストプラクティスである。

## なぜALBではなくNLBなのか？

### Global Acceleratorの仕組み

世界中のユーザーからのトラフィックをAWSグローバルネットワークで最適なエッジに引き込み、その後リージョン内のエンドポイント（ALBやNLBなど）に転送する。

ALBを直接つなぐことも可能だが、ALBはL7（HTTP/HTTPS）のみ対応のため、Global Acceleratorが持つTCP/UDPレベルの最適化をフルに活かせない。

### NLBを使うメリット

- L4（TCP/UDP）で処理 → 高パフォーマンス、低レイテンシ
- Global Acceleratorとの統合が最適（高可用性・高速化を最大限に活かせる）

---

# リードレプリカとリーダーエンドポイント

## リードレプリカ (Read Replica)

RDSの機能で、マスターDBからデータを非同期で複製する仕組み。主に読み取り負荷分散のために使う。

### 特徴
- 最大15個まで作成可能（DBエンジンによる）
- 書き込みはできず、読み取り専用
- 障害時に昇格させてプライマリにすることも可能（手動または自動フェイルオーバー設定時）

リードレプリカ = 実際に存在する「読み取り専用のDBインスタンス」

## リーダーエンドポイント (Reader Endpoint)

Aurora専用の機能（RDS MySQL/PostgreSQLにはない）。

Auroraクラスターには複数のリーダーノード（リードレプリカ）が存在する。リーダーエンドポイントを使うと、自動的に複数のリーダーノードに読み取りリクエストを振り分けてくれる。

### 特徴
- アプリ側は1つのエンドポイントを指定するだけでOK
- Auroraが裏でロードバランシング
- 書き込みリクエストは受け付けない

リーダーエンドポイント = Auroraクラスター内の複数リードレプリカに振り分ける「入り口」

「リーダーエンドポイント」と出てきたらAurora専用の機能と覚える。

---

# DAXクラスタの構成

## 構成要素

### プライマリノード（1つ必須）
- 書き込みリクエストを処理
- データの整合性を担保

### リードレプリカノード（0〜9個）
- 読み取りリクエストを処理
- 読み取りスケーリングを実現

つまりDAXクラスタ = 1プライマリ + 最大9リードレプリカ = 最大10ノードで構成可能である。

## フェイルオーバー

プライマリノードが障害を起こした場合、自動的にリードレプリカの1つがプライマリに昇格する。

アプリ側はクラスターエンドポイントに接続するため、個別のノードを意識する必要はない。

## まとめ

- DAXクラスタ構成：1プライマリ + 0〜9リードレプリカ
- 最大10ノードまで
- 自動フェイルオーバー & 読み取り負荷分散が可能
- アプリは「DAXクラスタのエンドポイント」に接続するだけで利用できる

---

# IoTデータパイプライン

```
IoTセンサー
↓
AWS IoT Core (MQTT)
↓
Amazon Kinesis Data Streams (リアルタイムストリーミング)
↓
Amazon Kinesis Data Firehose (データ変換・配信)
├── AWS Lambda (データ変換処理)
└── Amazon S3 (Parquet形式で保存)
    └── Amazon Glacier (長期保存)
```

## Firehose標準変換でできること

- JSON → Parquet/ORC変換
- 圧縮 (GZIP, Snappy等)
- 形式変換 (CSV → JSON等)
- スキーマレジストリ連携
- 基本的なカラム抽出

## Lambdaでしかできない処理

複雑なデータクレンジング

「データ形式をParquet/ORCにしたい」程度ならFirehoseだけでOK

---

# クエリによるデータ解析

Flinkは内部でSQLライクなクエリをサポートしており、ストリーミングデータに対して「窓（window）」を切った集計やフィルタリングが可能である。