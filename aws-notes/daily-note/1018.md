# AWS Transfer Family

## 概要
AWS Transfer Family（SFTP / FTPS / FTP）は、**フルマネージドなサーバレス転送サービス**で、以下の特徴を持つ。

## 特徴

### 1. マルチAZで自動冗長化
- サービス自体は複数のAZ（Availability Zone）に分散配置される
- AZ障害が起きても、SFTPエンドポイントは自動的に他のAZで継続稼働する
- ユーザーがEC2のように「どのAZに配置するか」設定する必要はない

### 2. スケーラブルでオートスケーリング
- 接続数や転送量が増えても、裏側でAWSが自動的に水平スケールする
- ユーザーは「サーバー台数」や「スケーリング設定」を意識しない
- 毎秒〜数千件のSFTP転送にも対応可能

### 3. 高可用性を意識したドメイン構成
- エンドポイント（例: s-xxxxxxxx.server.transfer.ap-northeast-1.amazonaws.com）自体が複数AZにまたがって冗長化されている
- Route 53のALIASレコードでFQDNを向けるだけで高可用性が確保される

---

# AWS AppSync

## CloudFrontとの比較
- CloudFrontは「静的キャッシュ配信」には有効だが、コメントのような動的・頻繁更新データには不向き
- AppSyncはGraphQL Subscriptionでリアルタイム更新可能

## アーキテクチャ

```
クライアント → AppSync (GraphQLエンドポイント)
           ├── DynamoDB (直接接続)
           ├── Lambda (関数呼び出し)
           ├── HTTP REST API (既存API呼び出し)
           └── その他 (RDS, Elasticsearch etc.)
```

- GraphQLを導入しても、REST APIを置き換える必要はない
- AppSyncは**GraphQLを使う"ゲートウェイ"**であり、REST APIを内部データソースとして統合できる

## リアルタイム更新のフロー

```
クライアント
   ↓（GraphQL Mutation）
AppSync
   ↓（書き込み）
DynamoDB
   ↓（変更検知）
DynamoDB Streams
   ↓（トリガー）
AppSync（GraphQL Subscription経由で通知）
   ↓
クライアントが即時更新を受信
```

## 役割

### ① GraphQLエンドポイントとしての「APIゲートウェイ」
- クライアントはAppSyncに対してGraphQLクエリ（Query/Mutation/Subscription）を投げる
- RESTのAPI Gatewayと違って、複数のデータ取得・更新を1リクエストでまとめられる

**例：**
- コメント投稿 → GraphQLのmutation
- コメントの購読（表示更新）→ GraphQLのsubscription
- REST API Gatewayのような「HTTPリクエストの受付口」だが、GraphQLで表現されているだけ

### ② リアルタイム通信の「サブスクリプションハブ」
- AppSyncにはGraphQL Subscriptionという機能があり、DynamoDB Streamsのイベント（データ更新）をサーバレスにPush配信できる
- クライアントはAppSyncに「コメント更新イベントを購読（subscribe）」しておくと、DynamoDBの変更が発生した瞬間にAppSyncからWebSocket経由で通知が飛ぶ
- WebSocketサーバーを自前で立てる必要がない
- 数十万同時接続もAppSyncがマネージドで処理

### ③ 認証・スケーリング・接続管理を自動で行う
- 接続維持（WebSocket）や再接続管理をAWSが全自動で処理
- 追加認証は不要（IAM・APIキーだけでOK）
- 数十万クライアントが接続してもスケーリング自動で、LambdaやEC2でセッション管理しなくていい
- 「リアルタイム配信の仕組み（接続・通知）」を完全にマネージドで提供

## まとめ
AppSyncは「GraphQLベースのリアルタイムAPIゲートウェイ」であり、DynamoDB Streamsとクライアント間の即時通信をマネージドに仲介する役割。

---

# AWS Application Discovery Service（ADS）

## 概要
AWS Application Discovery Service（ADS）は、オンプレミスのサーバー群をAWSに移行する前に、自動で構成情報を収集・可視化するためのサービス。

## 利用シーンの代表例
- オンプレ環境の全サーバーを一覧化したい
- Windows / Linux混在で、構成ドキュメントが古い or 不明
- 「どのサーバーをAWSへ移行すべきか」「どれを廃止すべきか」を分析したい
- AWS移行前にTCO試算や**7R戦略（Rehost / Replatform / Refactorなど）**を検討したい

## 収集方法の2パターン

AWS ADSは、環境に応じて次の2種類の方法で情報を収集できる。

### ① エージェントベース（Agent-based）
- 各オンプレサーバー（Windows/Linux）にエージェントをインストール
- より詳細な情報（プロセス・ポート・ソフトウェア・通信関係など）を取得可能
- 1時間ごとにAWSにメタデータを送信

**取得できる主な情報：**
- OSバージョン
- CPU・メモリ・ディスク使用率
- 開放ポートと通信先IP
- 稼働中のプロセス・サービス
- インストール済みソフトウェア一覧
- 稼働時間帯・負荷傾向

**特徴：**
- 精度が高いが、サーバーにインストール作業が必要

### ② エージェントレス（Agentless / Collector）
- VMware vCenter環境向け
- 管理ホスト（Collector VM）をデプロイして、vCenter経由でメタデータを取得
- 個々のサーバーにエージェントを入れる必要がない

**取得できる主な情報：**
- 仮想マシン一覧
- OS種別・CPU・メモリ・ディスク構成
- ネットワーク使用量
- 稼働時間・リソーストレンド

**特徴：**
- 詳細なアプリ情報までは取れない（例：ソフトウェア名、ポートなど）

## 収集データの活用先：AWS Migration Hub

収集した情報はAWS Migration Hubに送られて、そこで以下が可能：
- サーバーインベントリの一覧化
- 依存関係（通信関係）の可視化
- TCO試算レポート生成
- 7R分析（再ホスト・再構築など）検討に活用

つまり、Discovery Serviceは"現状調査"フェーズ、Migration Hubは"分析・計画"フェーズを担っている。

---

# RDS Enhanced Monitoring と Performance Insights

## Enhanced Monitoring（拡張モニタリング）

### 概要
RDSの**Enhanced Monitoring（拡張モニタリング）**は、OSレベルのメトリクス（CPU、メモリ、ディスクI/O、プロセスなど）をRDS内部の「CloudWatchメトリクス」よりも高頻度で取得できる機能。

### 特徴
- CloudWatchの通常メトリクスは最短1分間隔
- 拡張モニタリングではRDSインスタンス内のOS（実体のEC2）から直接データを収集するため、もっと細かい粒度（1秒単位）で観測できる

## 機能比較

### 基本比較

| 機能名 | 目的 | 粒度 | 主な用途 |
|--------|------|------|----------|
| Enhanced Monitoring（拡張モニタリング） | OSレベル（CPU・メモリ・I/O）の監視 | 最短1秒 | インスタンス全体のリソース使用状況を把握 |
| Performance Insights（パフォーマンスインサイト） | DBエンジン内部の負荷分析（SQLレベル） | 最短1秒（エンジンによる） | SQL・セッションごとのボトルネック可視化 |

### 詳細比較

| 比較項目 | Enhanced Monitoring | Performance Insights |
|----------|---------------------|----------------------|
| 監視レイヤー | OSレベル（EC2相当） | DBエンジン内部 |
| 最短間隔 | 1秒 | 1秒〜（DBエンジン依存） |
| 主要目的 | システムリソースの監視 | SQL・セッション負荷の分析 |
| 分析内容 | CPU / メモリ / I/O など | SQL文 / ユーザー / 待機イベントなど |
| 可視化場所 | CloudWatch Logs / メトリクス | RDSコンソールの Performance Insights 画面 |
| コスト | CloudWatch Logsに依存 | 7日無料、延長保存は有料 |

## よくある使い分け

| シナリオ | 使う機能 |
|----------|----------|
| CPU使用率が高い／I/O遅延を見たい | Enhanced Monitoring |
| どのSQLが重いか／ロックが発生しているかを調べたい | Performance Insights |
| 「CPU高負荷 → SQL特定まで」見たい | 両方併用（ベストプラクティス） |

---

# ElastiCache マルチAZ対応

## Redisの場合（マルチAZ = ほぼ標準構成）

### 対応内容
- レプリケーショングループ（Replication Group）機能により、プライマリノード＋リードレプリカを異なるAZに自動配置できる
- 障害発生時には、AWSが自動でフェイルオーバー（昇格）を実施
- フェイルオーバー後はDNSのエンドポイントが自動で更新され、アプリは再接続のみで復旧

## Memcachedの場合（制限あり）
- Memcachedはステートレス（分散キャッシュ）なので、Redisのような「レプリカ」「フェイルオーバー」は存在しない
- 代わりに「ノードを複数AZに手動配置」することは可能
- ただし、ノード障害時は自動フェイルオーバーではなく、アプリ側でリトライ制御が必要

### AWSの推奨
- 高可用性を重視する場合は、MemcachedよりもRedisを使用する
- Redisは自動フェイルオーバーを備えているため

---

# セッションデータとElastiCache

## セッションデータの特徴

セッションデータのような「一時的」「頻繁アクセス」「軽量」なデータなら、RDSなどの永続DBを使わずにElastiCache（特にRedis）で十分。

| 特徴 | 内容 |
|------|------|
| 短命 | 数分〜数時間で失効（TTLあり） |
| 頻繁アクセス | 認証・状態確認で毎リクエスト読み書きされる |
| 同時接続多い | 高トラフィックWebアプリでは秒間数千リクエストもあり |
| 一貫性より速度重視 | 読み込みの速さ・スケーラビリティ優先 |

## ElastiCache（Redis）が得意

### Redisがセッション管理に向いている理由

#### ① 超高速（インメモリ）
- 全データがメモリ上にあり、読み書きはマイクロ秒オーダー
- RDS（ディスクI/Oあり）と比べると桁違いに速い

#### ② TTL付きキーで自動削除
- `SET key value EX 3600`のように有効期限付きで保存できる
- セッションタイムアウト処理が不要

#### ③ スケールアウトが容易
- シャーディングやクラスタリングを使えば、大規模な同時接続にも耐えられる
- RDSだと接続上限（コネクション数）にすぐ達するが、Redisは軽い

#### ④ マルチAZフェイルオーバー対応（Redis）
- 冗長化・フェイルオーバーもAWSが自動で実施
- セッションデータが多少揮発しても再ログインで復元できるから、問題になりにくい

---

# Sticky Session（スティッキーセッション）

## 前提：普通のロードバランシングだと？

たとえばALB（Application Load Balancer）で負荷分散していると、デフォルトではリクエストごとに異なるバックエンドEC2（アプリサーバー）に振り分けられる。

```
ユーザーA → EC2-1
次のリクエスト → EC2-2
```

このとき、もしアプリが「ローカルセッション（サーバー内メモリ）」を使っていたら、別のサーバーに行った瞬間にセッション情報が消える（ログアウト状態になる）。

## Sticky Sessionとは

それを防ぐために：

**「同じセッションIDのリクエストは常に同じターゲットに送る」**

ようにするのがスティッキーセッション（英語: Session Affinity）。

## AWSでの実装例

AWS Application Load Balancer（ALB）やClassic ELBには、「スティッキーセッション（Cookieベース）」の設定がある。

### ALBの場合
- `AWSALB`というCookieがレスポンスに付与される
- クライアントがそのCookieを持って次のリクエストを送ると、ALBは同じターゲットにルーティングする

## 代表的なユースケース

| ユースケース | 理由 / メリット |
|--------------|-----------------|
| セッション情報をサーバーメモリに保持しているアプリ | 別サーバーに振られるとログアウトになる。スティッキーで回避 |
| 一時的なユーザーステートを持つアプリ（ゲーム・ショッピングカートなど） | 状態が途切れず、同一サーバーで継続可能 |
| 特定ユーザーに処理を固定したいケース | 例：大規模ファイルアップロード、チャット接続など。途中でサーバーが変わると処理が壊れる |

## スティッキーセッションのデメリット

| 問題点 | 内容 |
|--------|------|
| 負荷が偏る（スケール効果が下がる） | 同じユーザーが常に同じサーバーに行くため、均等な分散にならない |
| サーバーダウン時にセッション喪失 | セッションがそのサーバーだけに保存されている場合、障害時にユーザーがログアウトする |
| 水平スケールとの相性が悪い | オートスケーリングでサーバーが増減すると、セッション固定が破綻する |

## ベストプラクティス

スティッキーセッションは「一時的な回避策」と考える。

本番ではできるだけ、
- セッション情報を外部ストア（Redis / DynamoDB / ElastiCacheなど）に保存して、
- どのサーバーに振られても共通のセッションを参照できる構成にするのがベター

Redisでセッションを管理するメリットは、どのアプリサーバーにリクエストが行っても、共通のセッション情報を参照できることもある。

---

# DynamoDB Accelerator（DAX）

## 概要
- DynamoDB専用のマネージドキャッシュ
- TTL付きのインメモリキャッシュで、リード負荷を最大10倍削減
- 透過的にAPI互換

## ① DAX（DynamoDB Accelerator）を導入する

最強の即効策。

- DynamoDB専用のマネージドキャッシュ（インメモリ）
- 通常のDynamoDBクライアントの代わりに「DAXエンドポイント」を叩くだけでOK
- キャッシュヒット時は**数百マイクロ秒（µs）**でレスポンス

### 効果

| 状況 | レイテンシ |
|------|-----------|
| 通常DynamoDB | 5〜10 ms |
| DAXキャッシュヒット時 | 0.2〜0.8 ms |

## ② ElastiCache（Redis/Memcached）を併用

より柔軟で、他サービスとも共有キャッシュできる。

- DAXより自由度が高い（データ構造やTTLの制御など）
- 例えば「最近アクセスされた商品情報」などをキャッシュして、DynamoDBアクセスを減らす
- TTL（例：60秒）を設定して、DynamoDBを"たまにしか叩かない"構成にする

### 効果
キャッシュヒット率80%超なら、DynamoDBのReadレイテンシは実質ゼロに近づく。

「DBクエリの結果」や「APIレスポンス」など、"取得にコストがかかる結果"を一時保存して、次回以降はRedisから即返す考え方。

---

# Redisの役割とアプリの役割の線引き

| 領域 | 誰が決める？ | 説明 |
|------|-------------|------|
| どのデータをキャッシュする？ | アプリ側 | 例：人気商品、検索結果、セッションなど |
| どのタイミングでキャッシュに入れる？ | アプリ側 | 例：クエリ結果を取得後にset()する |
| どのくらいの期間保持する？（TTL） | アプリ側 | setex(key, ttl, value)で指定 |
| どのタイミングで削除する？ | アプリ側 | DB更新イベント後にdel(key)など |
| どうやって保存・取得する？ | Redis | 超高速なGET/SETを提供 |

---

# API GatewayのAPIキー

## 制約
- APIキーのメタデータは「デプロイ時に固定」される
- 一度設定したら、リクエストごとに書き換える仕組みは存在しない

## 書き込み用APIが存在しない
- Lambdaなどから「APIキーに動的データを書き込む」ことはできない

## まとめ
「API GatewayのAPIキーは、あくまで"誰が使ってるか"を識別する静的情報。"誰が今接続してるか"みたいな動的情報を入れる場所じゃない。」

---

# Redis vs Memcached：スレッドモデル

## 基本比較

| 項目 | Redis | Memcached |
|------|-------|-----------|
| スレッドモデル | シングルスレッド | マルチスレッド |
| 同時処理 | 1 CPU コアで逐次処理 | 複数コアを使って同時処理 |
| スケール方法 | 複数プロセス・クラスタリングで対応 | スレッドで同時アクセス処理 |
| 特徴 | 安定・予測しやすい性能 | 高スループット・CPU利用効率が高い |
| 向いてる用途 | キャッシュ＋永続化、キュー、Pub/Sub | 純粋なキャッシュ（短期メモリ保存） |

## Redisの場合（シングルスレッド中心）
- Redisは基本的に1スレッドでコマンド実行するので、1ノードにvCPUが16個あっても、使うのは1個だけ

## Memcachedの場合（マルチスレッド）
- Memcachedは最初からスレッド並列設計
- つまり、1ノード内で複数vCPUをフル活用してくれる
- 1台にコアが多い（＝マルチスレッド強い）環境では、MemcachedのほうがCPUリソースを無駄にせず使い切れる

---

# AWS ElastiCacheの場合で考えると

## スレッドモデルとスケール比較

| 項目 | Redis | Memcached |
|------|-------|-----------|
| スレッドモデル | シングル（＋I/Oのみマルチ） | フルマルチスレッド |
| vCPUスケール | 1コアしか使えない（Clusterでスケール） | 全vCPU使う（垂直スケールも効く） |
| スケーリング戦略 | 水平スケール（ノード分割） | 垂直スケール（vCPU増やす）も可 |
| 主な用途 | 状態管理・ランキング・セッション | 単純キャッシュ・一時データ |

## スケール方向の比較

| 比較項目 | Redis Cluster（水平スケール） | Memcached（垂直スケール） |
|----------|-------------------------------|---------------------------|
| スケール方向 | 横にノードを増やす（分散） | 1ノードのCPUやRAMを増やす |
| スレッド数 | 基本シングルスレッド | マルチスレッド |
| 分散方法 | ハッシュスロットでキーを分割 | ノード間でデータ複製なし（クライアント分散） |
| 主な用途 | 大量データや状態管理 | 単純キャッシュ・読み込み高速化 |
| 耐障害性 | レプリカあり（自動フェイルオーバー） | レプリカなし（シンプル構成） |
| 管理の複雑さ | やや複雑（クラスタリング） | シンプル（スケールアップ中心） |

---

# Memcached（垂直スケール）

## 特徴
Memcachedはマルチスレッド対応なので、1ノードにvCPUをたくさん積めばその分速くなる。

```
         ┌──────────┐
         │ Memcached │
         └──────────┘
              │
         ┌───────────────┐
         │  8 CPU cores   │
         │  64 GB Memory  │
         └───────────────┘
```

全CPUコアが同時にキャッシュリクエストを処理できる。
→ 1台あたりの処理能力をガッツリ引き上げ可能。

## メリット
- 設計がシンプルで導入が容易
- マルチコアを最大限活用できる
- レイテンシが非常に低い

## デメリット
- 冗長化・フェイルオーバー機能なし
- データ永続化不可（キャッシュのみ）
- ノード障害時はデータ全消失

RedisのINCRBYコマンド（カウンターを加算するコマンド）はアトミック操作（atomic operation）。
つまり、「他の処理が途中で割り込めない」＝同時更新でも整合性が保たれる。

メモリ内ストア = データをRAMに置いて、高速で読める一時的なデータベース

比較項目	メモリ内ストア（Redis/Memcached）	永続DB（RDS/DynamoDB）
保存場所	メモリ（RAM）	ディスク（SSD/HDD）
アクセス速度	μs〜ms（超高速）	ms〜数十ms
永続性	揮発性（Redisのみオプションあり）	永続的
主な用途	キャッシュ、セッション、リアルタイム集計	トランザクション、履歴データ保存

Redis Cluster
├── Shard 0  ←＝ ノードAが担当
│    ├─ Slot 0〜5460
│
├── Shard 1  ←＝ ノードBが担当
│    ├─ Slot 5461〜10922
│
└── Shard 2  ←＝ ノードCが担当
     ├─ Slot 10923〜16383

Redisクラスタ全体では、スロット番号 0〜16383（計16,384個） の空間が存在します。
このスロットを複数ノードに分割して割り当てることで、「シャーディング（sharding）」になります。

用語	意味	備考
スロット（Slot）	キー空間を細かく分割した単位（0〜16383）	CRC16(key) % 16384 でどのスロットに入るか決まる
シャード（Shard）	複数スロットをまとめたグループ	通常は1ノード＝1シャード
ノード（Node）	実際のRedisサーバー（プロセス）	シャードをホストしている実体

スロット番号	割り当てノード	中に保存されているキー
0	ノードA	user:1, user:2, user:3, …
1	ノードA	session:aaa, session:bbb
5461	ノードB	article:100, article:101
10923	ノードC	config:site_name, config:theme

セントラルEgress VPCの目的

大規模な環境では、各VPCが直接インターネットに出ると以下の問題があります。

セキュリティポリシーがVPCごとにバラバラになる

監査ログを一元的に取れない

ファイアウォールやNAT Gatewayの管理コストが増える

これを解決するために、
**「すべてのVPCのインターネット通信を1つのVPCに集約」**するのが「セントラルEgress VPC」です。

 [VPC-A]   \
 [VPC-B] --- (Transit Gateway) --- [セントラルEgress VPC] --- Internet
 [VPC-C]   /

各VPC → Transit Gateway 経由でセントラルEgress VPCへ

セントラルEgress VPC → NAT GatewayやFirewall経由でインターネットへ

この設計のメリット
項目	メリット
セキュリティ統制	中央のファイアウォールでポリシーを一元管理
コスト効率	各VPCにNAT Gatewayを置く必要がない（1箇所に集約）
運用性	ログや監査を一元化できる
スケーラビリティ	Transit Gatewayで数百〜数千VPCの接続に対応

ファイアウォールルールを1か所で集中管理

Outbound通信のログ・監査を統一的に取得

URLフィルタリングやIDS/IPSを中央で実施

どのVPCから、どんな外部サイトへ通信したか」を全体で可視化・制御できる

コスト最適化（NAT GatewayやFirewallを共有）

通常、各VPCにNAT Gatewayを置くとコストが高くつきます。
（NAT Gatewayは1台あたり固定費＋データ転送費が発生）

Egress VPCを設けて1か所にまとめると：

各VPCにNATを置かなくて済む

Firewall Applianceも1か所に設置で済む

結果として、数十〜数百のVPCを集約してもコストを抑えられるようになります。

Outbound通信の統制（出口制御）

全VPCのインターネット通信をEgress VPC経由にすれば、
**「許可された出口からしか通信できない」**状態を作れます。

たとえば：

App VPC → TGW → Egress VPC → Firewall → Internet

Firewallで宛先FQDN/IP制限・ログ収集・DLP対策などが可能

これにより、内部からのデータ流出リスクやマルウェア通信を防ぐことができます。

監査・可視化を一元化できる

セントラルEgress VPCを経由させることで、
CloudWatch Logs、GuardDuty、VPC Flow Logs などの監査基盤を1か所に統合できます。

これにより：

通信ログを集約・分析しやすくなる

SIEM連携（例：Splunk, Security Hub）も容易になる

各VPCの開発チームがログ管理から解放される

「ネットワーク専用アカウント」とは

AWSでは、マルチアカウント運用をする場合に役割ごとにアカウントを分けます。

アカウント種別	主な役割
管理アカウント（root / Organizations管理）	組織のルート管理
セキュリティアカウント	GuardDuty, Security Hub, Logsなどの集約
ネットワークアカウント	Transit Gateway, VPC, NAT Gateway, Network Firewall などネットワーク関連の共有リソースを集中管理
ワークロードアカウント	各チーム・アプリがEC2やLambdaを動かす場所

ではどうやって運用するのか？

「直接ログイン禁止」でも、ネットワークは運用しなければなりません。
そのために次のような方法を使います。

方法①：Infrastructure as Code（IaC）

AWS CloudFormation / Terraform / CDK などでネットワークリソースを管理

パイプライン（CI/CD）経由で自動デプロイ
→ 人間がログインせずにネットワークを更新できる

方法②：Delegated Access（委任アクセス）

AWS Organizations や IAM Role を使って、セキュリティチームだけが一時的に AssumeRole で入る
→ 監査ログがCloudTrailに残り、安全に作業可能

方法③：Service Catalog や Control Tower 経由で自動作成

新しいワークロードアカウントを作るときに、自動的にVPC共有設定を適用
→ ネットワークチームは何もしなくても反映される

「直接ログイン禁止」とは	人がAWSコンソールやCLIでログインして操作することを禁止する
運用方法	IaC・CI/CD・AssumeRoleを通してのみ操作する
目的	セキュリティと運用自動化、誤操作防止
背景	ネットワークアカウントは全社共通リソースを管理しているため、誤設定の影響が大きい

[Network Account]
     ├─ VPC（3 AZ）
     │    ├─ Subnet-AZ1（Shared）
     │    ├─ Subnet-AZ2（Shared）
     │    └─ Subnet-AZ3（Shared）
     └─ 共有設定（AWS RAM）

↓ 共有

[Workload Accounts]
     ├─ EC2 Instance → Shared Subnetに配置
     ├─ ENI, Security Groupなど一部リソースを自分のアカウントで管理
     └─ ネットワーク構成自体はNetwork Accountが管理

ネットワーク集中管理	VPC・ルート・NAT・Firewallはネットワーク専用アカウントで一元管理
セキュリティ分離	ワークロードアカウントはネットワーク構成に触れない（ログイン不要）
自動適用	AWS Organizationsの共有設定を使うことで新規アカウントにも自動適用可能
コスト効率	VPC/NATを共用するためリソースの重複がない
スケーラブル	数百アカウント規模でも設定が共通化できる

1. Shared VPC構成の基本

ネットワーク専用アカウント（ホストアカウント）
→ VPC・サブネット・ルートテーブル・NAT Gatewayなどを管理。

ワークロードアカウント（ゲストアカウント）
→ 共有されたサブネット内にEC2やECSなどのリソースを起動する。

これをAWS RAMで共有設定します。

2. Shared VPCを使うと何が起きるか

RAMで共有したサブネットを使うと、
ゲストアカウント（ワークロード側）が自分のアカウントから直接EC2を起動できるようになります。

つまり：

EC2の作成操作 → ワークロードアカウントで実施

そのEC2が配置されるVPCやサブネット → ネットワークアカウントのもの

ネットワークチームは → 「共有設定」を1回しておくだけ

3. ログインが不要になる仕組み

Shared VPCでは次のような分離が行われます。

操作	実行するアカウント	ログインの必要
EC2やECSをデプロイ	ワークロードアカウント	あり（通常操作）
サブネット作成・ルート設定・Firewall設定	ネットワークアカウント	なし（IaCまたは自動化）
VPC共有設定（最初の一度きり）	ネットワークアカウント	初回のみ設定、その後不要

4. 具体的な運用の流れ（例）

ネットワークチームがVPC・サブネットを作成

AWS RAMでOrganizations配下のすべてのアカウントに共有設定
（「自動共有」設定を有効にすれば新規アカウントにも自動反映）

アプリチームは自分のアカウントからEC2を起動
→ 共有されたVPCのサブネットを指定可能

ルート・Firewall・NATは中央で一元管理

AWS Resource Access Manager（RAM）によるVPC共有（Shared VPC）は「サブネット単位」で行われます。

つまり、VPC全体を共有するのではなく、VPC内の特定のサブネットを共有

[Network Account]（ホスト）
 └─ VPC（10.0.0.0/16）
     ├─ Subnet-AZ1（共有）
     ├─ Subnet-AZ2（共有）
     ├─ Route Table（ホストが管理）
     ├─ NAT Gateway, IGW
     └─ Firewall / TGW

[Workload Account]（ゲスト）
 └─ EC2, ECS, ALB
     └─ Shared Subnet に配置（ネットワーク操作不可）

アプリチームの責任範囲

EC2 / ECS / ALB / Lambda などアプリ資産の構築

セキュリティグループやIAMロールの管理

自分のアプリケーションの開発・運用

つまりShared VPCは、「ネットワークとアプリを完全に分離して、それぞれの担当チームが安心して作業できる環境」を作る仕組み